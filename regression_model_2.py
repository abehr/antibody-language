# -*- coding: utf-8 -*-
"""AntibodyBindingEnergyRegressionModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N2t4JCxG1uFA__bM3Hu4RoD6_Dvhn_sP
"""
print('AbReg - Run regression model')

import time
import os
import numpy as np
import torch
import torch.nn as nn
import keras
from keras.models import Sequential
from keras.layers import Dense
import workflow
from embedding_generator import EmbeddingGenerator

name = 'seq85k'
use_cpu = False

# df = workflow.import_energy_metadata()
foldx_dict = workflow.import_energy_metadata_foldx()
seqs = workflow.get_embedding_list(name)
np.random.shuffle(seqs) # ensure random order

input_shape = 585782

dropout = .2

def RegressionModel():
    X_input = keras.Input(shape=input_shape)

    # X = keras.layers.Lambda(lambda batch: workflow.load_embeddings(name, batch))(X_input)
    X = keras.layers.Dropout(dropout)(X_input)

    for _ in range(8):
        X = Dense(800, activation='relu', kernel_initializer="he_uniform")(X)
        X = keras.layers.BatchNormalization(axis=-1, momentum=0.99)(X)
        X = keras.layers.Dropout(dropout)(X)

    X = Dense(700, activation='relu', kernel_initializer="he_uniform")(X)
    X = keras.layers.BatchNormalization(axis=-1, momentum=0.99)(X)
    X = keras.layers.Dropout(dropout)(X)


    X = Dense(400, activation='relu', kernel_initializer="he_uniform")(X)
    X = keras.layers.BatchNormalization(axis=-1, momentum=0.99)(X)
    X = keras.layers.Dropout(dropout)(X)


    X = Dense(2, kernel_initializer="he_uniform")(X)

    model = keras.Model(inputs = X_input, outputs = X, name='RegressionModel')

    return model

print('AbReg - Initialize model')
model = RegressionModel()

print('AbReg - Compile model')
model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())




batch_size = 256 # 16-1024

# smaller subset for testing
train_data = EmbeddingGenerator(name, seqs[:80000], foldx_dict, batch_size)
valid_data = EmbeddingGenerator(name, seqs[80000:81000], foldx_dict, batch_size)
test_data = EmbeddingGenerator(name, seqs[81000:82000], foldx_dict, batch_size)


# train_data = EmbeddingGenerator(name, seqs[:-2000], foldx_dict, batch_size)
# valid_data = EmbeddingGenerator(name, seqs[-2000:-1000], foldx_dict, batch_size)
# test_data = EmbeddingGenerator(name, seqs[-1000:], foldx_dict, batch_size)

print('AbReg - Train model')
model.fit(train_data, validation_data=valid_data, epochs=1)

timestamp = time.strftime('%m%d_%H%M', time.localtime(time.time()))
model_fp = os.path.join('models', 'abreg_' + timestamp)

print('AbReg - Save model to file %s' % model_fp)
model.save(model_fp)


print('AbReg - Evaluate model')
model.evaluate(test_data)

# traintest = EmbeddingGenerator(name, seqs[500:600], foldx_dict, batch_size)

# for subset_200_seq85k, try batch size = 16 
# 12/12 [==============================] - 5s 450ms/step - loss: 27.4618